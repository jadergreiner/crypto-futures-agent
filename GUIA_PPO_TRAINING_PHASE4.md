"""
GUIA PR√ÅTICO ‚Äî Como Executar Treinamento Phase 4 (23-27 FEV)
===============================================================

Checklist pr√©-treinamento, rotina di√°ria e revalida√ß√£o.
"""

# ============================================================================
# PARTE 1: PR√â-TREINAMENTO (23 FEV 10:00-14:00 UTC)
# ============================================================================

PRE_TRAINING_CHECKLIST = """
=== 23 FEV, 10:00 UTC: PR√â-TREINAMENTO CHECKLIST ===

[ ] 1. SWE finalizou integra√ß√£o
    - agent/trainer.py foi atualizado com PPOConfig (config/ppo_config.py)
    - ConvergenceDashboard est√° importado e funcional
    - Teste r√°pido: python -c "from config.ppo_config import PPOConfig; print(PPOConfig())"

[ ] 2. Dados de treinamento est√£o prontos
    - data/ tem H4, H1, D1, sentiment, macro, SMC
    - Cache de OHLCV est√° atualizado (~700 candles m√≠nimo)
    - Test: python -c "from data.data_loader import DataLoader; print('OK')"

[ ] 3. Environment testado
    - agent/environment.py retorna obs, done, info['trades'], info['capital']
    - Test: pytest tests/test_environment.py -v

[ ] 4. Diret√≥rios criados
    - mkdir -p logs/ppo_training
    - mkdir -p models/ppo_phase4
    - mkdir -p reports/revalidation

[ ] 5. Depend√™ncias verificadas
    - pip list | grep stable-baselines3
    - pip list | grep gymnasium
    - pip list | grep numpy

[ ] 6. TensorBoard (opcional)
    - pip install tensorboard (se quer visualizar)
    - ou usar CSV do dashboard

[ ] ‚úÖ PRONTO PARA INICIAR TREINAMENTO
"""

# ============================================================================
# PARTE 2: INICIAR TREINAMENTO (23 FEV 14:00 UTC)
# ============================================================================

START_TRAINING_COMMANDS = """
=== 23 FEV, 14:00 UTC: INICIAR TREINAMENTO ===

# Op√ß√£o A: Script simples (recomendado)
python scripts/start_ppo_training.py

# Op√ß√£o B: Manualmente no Python
python
>>> from config.ppo_config import PPOConfig
>>> from agent.trainer import Trainer
>>> from scripts.ppo_training_dashboard import ConvergenceDashboard
>>> from data.data_loader import DataLoader

>>> config = PPOConfig.phase4_conservative()
>>> trainer = Trainer(save_dir="models/ppo_phase4")
>>> dashboard = ConvergenceDashboard(log_dir="logs/ppo_training")
>>> data = DataLoader.load_backtest_data("BTCUSDT")

>>> # Treinar com dashboard
>>> trainer.train_with_dashboard(
...     train_data=data,
...     config=config,
...     dashboard=dashboard,
...     total_timesteps=500_000
... )

# Monitorar em tempo real
tail -f logs/ppo_training/convergence_dashboard.csv
tail -f logs/ppo_training/daily_summary.log
"""

# ============================================================================
# PARTE 3: MONITORAMENTO DI√ÅRIO (23-27 FEV, 10:00 UTC cada dia)
# ============================================================================

DAILY_CHECKIN_TEMPLATE = """
=== DAILY CHECK-IN TEMPLATE (10:00 UTC) ===

Data: __/__/2026

1. M√âTRICAS ATUAIS (do CSV ou dashboard log)
   - Episodes trained hoje: ___
   - Reward moving average (50 ep): ___
   - Best episode reward: ___
   - Current Sharpe estimate: ___
   - KL divergence (√∫ltimas updates): ___

2. SA√öDE DO TREINAMENTO
   - Converg√™ncia? (reward aumentando?)  [ ] Sim [ ] Lento [ ] Plateau
   - Nenhum crashe ou erro?              [ ] Sim [ ] N√£o ‚Üí PARAR E DEBUG
   - Entropy normal (0.0001-1.5)?        [ ] Sim [ ] Anormalmente baixa [ ] Anormalmente alta
   - Gradient norm < 1.0?                [ ] Sim [ ] Explos√£o detected

3. ALERTAS GERADOS?
   - KL divergence > 0.05?  [ ] N√£o [ ] Sim ‚Üí reduzir learning rate?
   - No improve x 100 ep?   [ ] N√£o [ ] Sim ‚Üí preparar parada
   - Sharpe > 0.7?          [ ] N√£o [ ] Sim ‚Üí SAVE CHECKPOINT IMMEDIATELY

4. A√á√ïES NECESS√ÅRIAS
   - [ ] Nenhuma (continuar)
   - [ ] Salvar checkpoint
   - [ ] Ajustar hyperpar√¢metro (LR, entropy)
   - [ ] Parar e investigar

5. ESTIMATIVA DE TEMPO RESTANTE
   - Horas de treinamento completadas: ___
   - Steps completados: _____ / 500k
   - Estimativa de conclus√£o: __:__ UTC em __/__/2026
"""

# ============================================================================
# PARTE 4: ROTINA DI√ÅRIA DETALHADA (por dia)
# ============================================================================

DAILY_ROUTINES = {
    "23_FEV": {
        "10:00": "‚úÖ CHECKLIST PR√â-TREINAMENTO (ver PARTE 1)",
        "14:00": "‚úÖ INICIAR TREINAMENTO (ver PARTE 2)",
        "14:30": "Verificar primeiros logs em logs/ppo_training/",
        "18:00": "Primeiro check-in r√°pido (epis√≥dios come√ßaram?)",
    },
    "24_FEV": {
        "10:00": "üìä DAILY CHECKIN #1 (ver PARTE 3)",
        "target": "Reward deve estar >-50 (m√≠nimo learning)",
        "action": "Se muito ruim (<-100): verificar environment, reward function",
    },
    "25_FEV": {
        "10:00": "üìä DAILY CHECKIN #2",
        "target": "Sharpe estimate > 0.2 (come√ßando a convergir)",
        "note": "Se ainda negativo: normal, modelo explore ainda",
        "action_ok": "Continuar normalmente",
        "action_bad": "Verificar reward clipping, nn architecture",
    },
    "26_FEV": {
        "10:00": "üìä DAILY CHECKIN #3",
        "target": "Modelo come√ßando a consolidar (reward est√°vel ou +)",
        "target_sharpe": "0.3-0.7 (bom sinal de converg√™ncia)",
        "action": "Se plateau: considerar parar treinamento cedo",
    },
    "27_FEV": {
        "10:00": "üìä DAILY CHECKIN #4 (√∫ltimo antes revalida√ß√£o)",
        "target": "Sharpe ‚â•0.7 esperado (ready for validation)",
        "16:00": "üî¨ EXECUTAR REVALIDA√á√ÉO (ver PARTE 5)",
        "17:00": "üìã GO/NO-GO DECISION",
    },
}

# ============================================================================
# PARTE 5: REVALIDA√á√ÉO (27 FEV 16:00 UTC)
# ============================================================================

REVALIDATION_STEPS = """
=== 27 FEV, 16:00 UTC: REVALIDA√á√ÉO COM 6 GATES ===

Step 1: Carregar melhor modelo treinado
    python
    >>> from scripts.revalidate_model.py import RevalidationValidator
    >>> validator = RevalidationValidator()
    >>> model, vec_norm = validator.load_model("best_model")
    
Step 2: Preparar dados de backtest (sem leakage)
    >>> from data.data_loader import DataLoader
    >>> backtest_data = DataLoader.load_validation_set()
    
Step 3: Executar backtest
    >>> trades, equity_curve, stats = validator.run_backtest(
    ...     model=model,
    ...     vec_normalize=vec_norm,
    ...     backtest_data=backtest_data,
    ...     num_episodes=10
    ... )
    
Step 4: Calcular 6 m√©tricas
    >>> metrics = validator.calculate_metrics_from_trades(trades, equity_curve)
    >>> print(metrics)
    
Step 5: Validar contra 6 gates
    >>> result = validator.validate_gates(metrics)
    >>> print(f"Gates passed: {result['gates_passed']}/6")
    >>> print(f"Decision: {result['go_no_go']}")
    
Step 6: Gerar relat√≥rio
    >>> report = validator.generate_report(result)
    >>> validator.save_results(result, report)
    
Step 7: Impacto
    >>> print(result['go_no_go'])
    "GO"        ‚Üí Proceder com 28 FEV deployment
    "PARTIAL"   ‚Üí CTO review necess√°rio
    "NO-GO"     ‚Üí Analisar, considerar Option A modificado

Expectativa: 5-6 / 6 gates (vs 2/6 random)
"""

# ============================================================================
# PARTE 6: TROUBLESHOOTING
# ============================================================================

TROUBLESHOOTING = """
=== TROUBLESHOOTING DURANTE TREINAMENTO ===

‚ùå Problema: "Reward n√£o est√° aumentando, fica em ~-50"
   ‚Üí Causa prov√°vel: Environment retornando rewards ruins constantemente
   ‚Üí A√ß√£o: Verificar reward.py, conferir se formula est√° correta
   ‚Üí Debug: python -c "from agent.environment import CryptoFuturesEnv; env = CryptoFuturesEnv(...); obs, info = env.reset(); print(info)"

‚ùå Problema: "Model crashes com CUDA error"
   ‚Üí A√ß√£o: Usar CPU em vez de GPU (ou ajustar batch_size)
   ‚Üí Config: config.ppo_config.PPOConfig() ‚Üí batch_size = 32 (reduzido)

‚ùå Problema: "Sharpe muito baixo depois de 5 dias (< 0.3)"
   ‚Üí Causa: Modelo n√£o convergedido com reward function atual
   ‚Üí A√ß√£o: Considerar Option A (override com heur√≠sticas) + continue training
   ‚Üí Ou Option B: Aumentar learning_rate ligeiramente (5e-4), continuar

‚ùå Problema: "KL divergence constantemente > 0.05"
   ‚Üí A√ß√£o: Pol√≠tica est√° mudando muito por update
   ‚Üí Fix: Reduzir learning_rate (1e-4), aumentar clip_range (0.3)

‚ùå Problema: "No improvement por 100+ epis√≥dios no meio do treinamento"
   ‚Üí Normal: Explora√ß√£o vs exploitation
   ‚Üí A√ß√£o: Aumentar ent_coef temporariamente (0.005)

‚úÖ Sucesso: "Sharpe > 0.7 no l√≥gos"
   ‚Üí Salvar checkpoint autom√°tico (ConvergenceDashboard faz isso)
   ‚Üí Status = "EXCELLENT" ‚Äî pronto para revalida√ß√£o

‚ùì D√∫vida: "Quantos epis√≥dios = quantos dias?"
   ‚Üí ~5k episodes / dia (com GPU parallelism)
   ‚Üí ~500k total timesteps / 5k = ~100 dias epis√≥dios
   ‚Üí Mas cada episode = 500 steps, ent√£o ~2000k / dia steps
   ‚Üí 500k / 2000k ‚âà 0.25 dias = 6 horas CPU
   ‚Üí Realista: 5-7 dias wall-clock com GPU
"""

if __name__ == "__main__":
    print(PRE_TRAINING_CHECKLIST)
    print("\n")
    print(START_TRAINING_COMMANDS)
    print("\n")
    print(DAILY_CHECKIN_TEMPLATE)

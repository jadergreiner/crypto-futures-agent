# ğŸ§ª PHASE 2: Core E2E Tests Execution

**Date:** 23 FEV 2205 UTC (22:05-01:35 = 4h)  
**Lead:** Quality (#12)  
**Support:** Arch (#6) code review, The Brain (#3) signal quality monitoring  
**Status:** ğŸŸ¡ SCHEDULED (depends on Phase 1 Go decision)

---

## ğŸ“‹ Test Execution Plan (4h SLA)

### Test Suite: 8/8 E2E Tests

```python
# tests/test_issue_66_smc_e2e_integration.py

## UNIT TESTS (Target: 15-20s)

Test #1: test_smc_signal_generation_e2e
  â”œâ”€ Input: 10 symbols Ã— 1Y data (from S2-0 cache)
  â”œâ”€ Process: indicators/smc.py â†’ volume threshold SMA(20) + order blocks
  â”œâ”€ Output: signals list, each signal has:
  â”‚  â”œâ”€ symbol
  â”‚  â”œâ”€ signal_type (order_block | bos | confluent)
  â”‚  â”œâ”€ confidence (0.0-1.0, target >0.7)
  â”‚  â”œâ”€ strength (volume ratio)
  â”‚  â””â”€ timestamp
  â”œâ”€ Expected: signal_count â‰¥ 50 (reasonable density)
  â”œâ”€ Assert: all signals confidence > 0.7 âœ…
  â”œâ”€ Duration: ~5s
  â””â”€ Status: ğŸŸ¡ TO-RUN

Test #2: test_order_executor_receives_smc_signals
  â”œâ”€ Input: Valid SMC signal from Test #1
  â”œâ”€ Process: execution/heuristic_signals.py._validate_smc()
  â”œâ”€ Output: validated signal (+order blocks confluence check)
  â”œâ”€ Expected: signal approved (passes all validations)
  â”œâ”€ Assert: heuristic_signals._validate_smc() returns True âœ…
  â”œâ”€ Duration: ~5s
  â””â”€ Status: ğŸŸ¡ TO-RUN

Test #3: test_risk_gates_active_with_smc
  â”œâ”€ Input: Position in paper mode + SMC signal
  â”œâ”€ Process: Order executor + risk gates (SL -3%, CB, TSL)
  â”œâ”€ Scenario: Simulate loss -3.1% â†’ expect CB close
  â”œâ”€ Expected: Position closed by circuit breaker
  â”œâ”€ Assert: position.status == 'CLOSED' âœ…
  â”œâ”€ Assert: close_reason == 'CIRCUIT_BREAKER' âœ…
  â”œâ”€ Duration: ~10s
  â””â”€ Status: ğŸŸ¡ TO-RUN

## INTEGRATION TESTS (Target: 45-60s)

Test #4: test_signal_generation_to_order_execution_e2e
  â”œâ”€ Full Flow: SMC signal gen â†’ heuristic validation â†’ order exec â†’ position monitor
  â”œâ”€ Input: 10 symbols, 1Y data
  â”œâ”€ Expected: Complete flow completes within latency SLA
  â”œâ”€ Timing Checkpoints:
  â”‚  â”œâ”€ signal_gen start: T0
  â”‚  â”œâ”€ signal_gen end: T0 + 50ms (target)
  â”‚  â”œâ”€ heuristic validation: T0 + 100ms (target)
  â”‚  â”œâ”€ order exec: T0 + 150ms (target)
  â”‚  â””â”€ position monitor: T0 + 250ms (target)
  â”œâ”€ Assert: latency < 250ms âœ…
  â”œâ”€ Duration: ~15s (per signal Ã— 5-10 signals)
  â””â”€ Status: ğŸŸ¡ TO-RUN

Test #5: test_edge_cases_gaps_ranging_lowliq
  â”œâ”€ Scenario A: Gap detection
  â”‚  â”œâ”€ Input: OHLCV with overnight gap
  â”‚  â”œâ”€ Expected: Gap detected, signal filtered
  â”‚  â””â”€ Assert: gap_signal rejected âœ…
  â”œâ”€ Scenario B: Ranging market
  â”‚  â”œâ”€ Input: range > 50% in last 1h
  â”‚  â”œâ”€ Expected: Ranging identified, signal marked uncertain
  â”‚  â””â”€ Assert: ranging_confidence < 0.5 âœ…
  â”œâ”€ Scenario C: Low liquidity
  â”‚  â”œâ”€ Input: volume < 10 BTC
  â”‚  â”œâ”€ Expected: Low-liq signal handled safely
  â”‚  â””â”€ Assert: low_liq_signal rejected âœ…
  â”œâ”€ Duration: ~20s (3 scenarios Ã— ~7s each)
  â””â”€ Status: ğŸŸ¡ TO-RUN

Test #6: test_latency_profile_98p
  â”œâ”€ Process: Run 100+ signal cycles, measure latency
  â”œâ”€ Metric: Calculate 98th percentile latency
  â”œâ”€ Expected: latency_98p < 250ms
  â”œâ”€ Output: {"mean": XXXms, "p50": XXXms, "p95": XXXms, "p98": XXXms}
  â”œâ”€ Assert: latency_98p < 250ms âœ…
  â”œâ”€ Duration: ~20s (100 cycles Ã— 0.2s overhead)
  â””â”€ Status: ğŸŸ¡ TO-RUN

## EDGE CASE + REGRESSION TESTS (Target: 30-45s)

Test #7: test_regression_sprint1_70_tests
  â”œâ”€ Command: `pytest tests/ -v --tb=short` (all Sprint 1 tests)
  â”œâ”€ Expected: ALL 70 tests PASS (0 failures)
  â”œâ”€ Timeout: 60s (conservative)
  â”œâ”€ Assert: test_count == 70, failures == 0 âœ…
  â”œâ”€ Duration: ~45s
  â””â”€ Status: ğŸŸ¡ TO-RUN

Test #8: test_regression_s24_50_tests
  â”œâ”€ Command: `pytest tests/test_s2_4*.py -v` (S2-4 TSL tests)
  â”œâ”€ Expected: ALL 50+ tests PASS
  â”œâ”€ Timeout: 60s
  â”œâ”€ Assert: test_count >= 50, failures == 0 âœ…
  â”œâ”€ Duration: ~30s
  â””â”€ Status: ğŸŸ¡ TO-RUN
```

---

## â±ï¸ Execution Timeline (4h = 240min)

```
22:05 â€” PHASE 2 KICKOFF
â”‚
â”œâ”€ 22:05-22:15 (10min): Setup + fixtures load
â”‚  â”œâ”€ Load test data (10 symbols Ã— 1Y)
â”‚  â”œâ”€ Initialize test fixtures
â”‚  â””â”€ Start CI/CD pipeline logging
â”‚
â”œâ”€ 22:15-22:35 (20min): UNIT TESTS (1-3)
â”‚  â”œâ”€ Test #1 (SMC signal gen): 5min
â”‚  â”œâ”€ Test #2 (Executor signal receive): 5min
â”‚  â”œâ”€ Test #3 (Risk gates): 10min
â”‚  â””â”€ Result: âœ… 3/3 PASS (target)
â”‚
â”œâ”€ 22:35-23:35 (60min): INTEGRATION TESTS (4-6)
â”‚  â”œâ”€ Test #4 (E2E signalâ†’exec): 15min
â”‚  â”œâ”€ Test #5 (Edge cases): 20min
â”‚  â”œâ”€ Test #6 (Latency profile): 20min
â”‚  â””â”€ Result: âœ… 3/3 PASS (target)
â”‚
â”œâ”€ 23:35-00:10 (35min): REGRESSION TESTS (7-8)
â”‚  â”œâ”€ Test #7 (Sprint 1 regression): 45min BUT PARALLEL
â”‚  â”œâ”€ Test #8 (S2-4 regression): 30min BUT PARALLEL
â”‚  â””â”€ Result: âœ… 70+ PASS + 50+ PASS
â”‚
â”œâ”€ 00:10-01:20 (70min): COVERAGE REPORT + BUFFER
â”‚  â”œâ”€ Generate coverage report: `pytest --cov=execution`
â”‚  â”œâ”€ Target: â‰¥85% coverage (execution/heuristic_signals.py)
â”‚  â”œâ”€ Contingency buffer for failures
â”‚  â””â”€ Re-run any failed tests
â”‚
â””â”€ 01:35: PHASE 2 COMPLETE
    â””â”€ Phase 2 Summary: 8/8 PASS? Yes/No? Coverage â‰¥85%?
        â””â”€ Go/No-Go Phase 3 decision
```

---

## ğŸ“Š Phase 2 Monitoring

### Real-Time Dashboard (to update every 15min)

```
Phase 2 Progress: â³ IN PROGRESS

Unit Tests (1-3):
  [ ] Test #1 (signal gen): â³ RUNNING
  [ ] Test #2 (exec receive): â³ QUEUED
  [ ] Test #3 (risk gates): â³ QUEUED
  Status: 0/3 PASS

Integration Tests (4-6):
  [ ] Test #4 (E2E flow): â³ QUEUED
  [ ] Test #5 (edge cases): â³ QUEUED
  [ ] Test #6 (latency): â³ QUEUED
  Status: 0/3 PASS

Regression Tests (7-8):
  [ ] Test #7 (Sprint 1): â³ QUEUED (runs parallel)
  [ ] Test #8 (S2-4): â³ QUEUED (runs parallel)
  Status: 0/2 PASS

Coverage:
  [ ] Pending (after Phase 2 complete)
  Target: â‰¥85%

Blockers:
  [ ] None identified
  
Latency Budget Used:
  [ ] Signal gen: TBD (target 50ms)
  [ ] Heuristic: TBD (target 50ms)
  [ ] Executor: TBD (target 100ms)
  [ ] Monitor: TBD (target 50ms)
  [ ] Total: TBD (target <250ms)
```

---

## ğŸ”´ Failure Scenarios & Recovery

### If Test #1 Fails (SMC signal gen)

**Issue:** Signal generation not producing expected signals  
**Recovery:**
```
1. Check test data: Verify 10 symbols loaded from S2-0 cache âœ…
2. Debug Issue #63: Review indicators/smc.py volume_threshold logic
3. Check thresholds: Confirm SMA(20) calculated correctly
4. Re-run with DEBUG logging enabled
5. If still fails: Escalate to Arch (#6) for code review
   â†’ Max 15min debug time, then pivot to Phase 3 (edge cases)
```

### If Test #4 Fails (E2E latency)

**Issue:** Latency > 250ms (98p)  
**Recovery:**
```
1. Profile each stage: signal_gen | heuristic | executor | monitor
2. Identify bottleneck (likely: signal_gen or executor)
3. If signal_gen: Check IO bottleneck (cache hit?)
4. If executor: Check Risk Gate evaluation (unnecessary loops?)
5. Optimization options:
   a) Cython compilation for hot paths
   b) Parallel signal generation (if safe)
   c) Caching optimization
6. Re-run latency profile
7. If still > 250ms at 98p: Escalate, document trade-off
   â†’ PPO may need real-time signal buffer (acceptable risk)
```

### If Regression Tests Fail (#7 or #8)

**Issue:** Sprint 1 or S2-4 tests broken by Issue #66 changes  
**Recovery:**
```
1. Identify which test broke: Sprint 1 (70 tests) or S2-4 (50 tests)
2. Isolate failure: Which module changed?
   - execution/heuristic_signals.py? (likely)
   - execution/order_executor.py? (possible)
   - risk/circuit_breaker.py? (less likely)
3. Revert problematic change (if introduced in Phase 2)
4. OR: Fix root cause (if Issue #63 integration issue)
5. Re-run regression suite
6. Max 20min debug time, else escalate to Arch (#6)
```

---

## ğŸ“ Escalation Matrix

| Issue | Severity | Owner | Max Resolution Time |
|-------|----------|-------|---------------------|
| Test #1-3 failure | ğŸ”´ CRITICAL | Arch (#6) | 15min |
| Latency > 250ms (98p) | ğŸŸ  HIGH | Arch (#6) | 30min |
| Regression fail (Sprint 1) | ğŸ”´ CRITICAL | Arch (#6) + Quality (#12) | 20min |
| Coverage < 80% | ğŸŸ¡ MEDIUM | Quality (#12) | Accept &continue |
| Any test timeout (>30s) | ğŸŸ  HIGH | Quality (#12) + Arch (#6) | 15min debug |

---

## âœ… Phase 2 Success Criteria

**All Must-Have (GO to Phase 3):**
- âœ… Tests #1-3: 3/3 PASS
- âœ… Tests #4-6: 3/3 PASS (or latency documented)
- âœ… Tests #7-8: 70+ PASS + 50+ PASS
- âœ… Coverage: â‰¥80% (target 85%)
- âœ… 0 CRITICAL blockers unresolved

**Go/No-Go Phase 3 Decision:** Arch (#6) + Audit (#8)

---

## ğŸ“Œ Phase 2 â†’ Phase 3 Handoff (01:35 UTC)

**Phase 3 Readiness:**
- [ ] All 8/8 core tests PASS âœ…
- [ ] Latency profiled âœ…
- [ ] Regression baseline established âœ…
- [ ] Coverage report ready âœ…
- [ ] Blockers resolved âœ…

**Phase 3 Kick-Off (01:35 UTC):**
```
Phase 3 Scope: Edge cases + latency optimization
  â”œâ”€ 60 symbols (vs 10 in Phase 2)
  â”œâ”€ Stress testing: gaps, ranging, low-liq extreme cases
  â”œâ”€ Latency optimization if needed
  â””â”€ Performance baseline validation
```

**Checkpoint: 05:30 UTC (after Phase 3)**
- Phase 3 Go/No-Go Phase 4 decision

---

**Phase 2 Status:** ğŸŸ¡ SCHEDULED (23 FEV 22:05-01:35)  
**Lead:** Quality (#12)  
**Support:** Arch (#6), The Brain (#3)

# ğŸ¤– TASK-005 ML Training Pipeline â€” PPO v0 Specification

**TÃ­tulo:** ML Training Pipeline (PPO) â€” S2-3 Sprint 2-3  
**Owner:** The Brain (#3) â€” ML/Arch  
**Timeline:** 22-25 FEV 2026  
**Deadline:** â° 25 FEV 10:00 UTC (HARD CONSTRAINT)  
**Duration:** 96 hours wall-time  
**Status:** ğŸš€ READY TO KICKOFF (Gate 3 âœ… unblocked)  

---

## ğŸ¯ Objetivo

Treinar modelo PPO (Proximal Policy Optimization) em histÃ³rico de trades de Sprint 1
para otimizar parametros de entrada da estratÃ©gia SMC (Order Blocks + BreakOfStructure).

**SaÃ­da esperada:** Policy treinada salva em `models/ppo_v0.pkl` com Sharpe â‰¥ 0.80

---

## ğŸ“‹ PrÃ©-requisitos âœ…

- âœ… **Backtest Engine (S2-3 Gate 2-3):** backtest/metrics.py + tests COMPLETE
- âœ… **Sprint 1 Data:** 70 trades histÃ³ricos em data/trades_history.json
- âœ… **Environment:** Python 3.9+, stable-baselines3, gymnasium (installed)
- âœ… **Risk Gate:** Risk module loaded (stop loss -3%, liquidation check)

---

## ğŸƒ Phase 1: Environment Setup (23 FEV 00:00-06:00 UTC)

**Owner:** The Blueprint (#7)  
**Tasks:**

### T5.1.1 â€” Criar CustomTrainingEnv

```python
# File: agent/rl/training_env.py

import gymnasium as gym
from gymnasium import spaces
import numpy as np
from backtest.metrics import MetricsCalculator

class CryptoTradingEnv(gym.Env):
    """
    Gym environment para treinamento PPO.
    
    ObservaÃ§Ã£o: estado do mercado + posiÃ§Ã£o aberta
    AÃ§Ã£o: HOLD, LONG, SHORT (3 aÃ§Ãµes)
    Recompensa: PnL realizado + bÃ´nus Sharpe
    """
    
    def __init__(self, trade_data, initial_capital=10000):
        super().__init__()
        self.trade_data = trade_data  # list of OHLCV
        self.initial_capital = initial_capital
        self.current_step = 0
        self.equity = initial_capital
        self.position = 0  # 0=close, 1=long, -1=short
        self.entry_price = 0
        
        # Observation space: [close, volume, rsi, position, pnl]
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(5,), dtype=np.float32
        )
        
        # Action space: HOLD=0, LONG=1, SHORT=2
        self.action_space = spaces.Discrete(3)
        
        self.trades_history = []
        
    def reset(self):
        """Reset environment to start of episode."""
        self.current_step = 0
        self.equity = self.initial_capital
        self.position = 0
        self.entry_price = 0
        self.trades_history = []
        
        obs = self._get_observation()
        return obs, {}
    
    def step(self, action):
        """Execute one trading step."""
        # TODO: Implementar lÃ³gica de execuÃ§Ã£o, PnL cÃ¡lculo, risk gate check
        # action: 0=HOLD, 1=LONG, 2=SHORT
        
        # Calcular reward (PnL realizado + bÃ´nus)
        reward = 0  # TODO: compute from position change
        
        # Check para risk gate (max drawdown, etc)
        risk_gate_breach = False  # TODO: check
        
        obs = self._get_observation()
        terminated = risk_gate_breach or self.current_step > len(self.trade_data)
        
        self.current_step += 1
        
        return obs, reward, terminated, False, {}
    
    def _get_observation(self):
        """Retorna observaÃ§Ã£o do estado do mercado + posiÃ§Ã£o."""
        if self.current_step < len(self.trade_data):
            candle = self.trade_data[self.current_step]
            close = candle['close']
            volume = candle['volume']
            # TODO: Calcular RSI ou outro indicador
            rsi = 50.0  # placeholder
            
            pnl = 0.0
            if self.position != 0:
                pnl = (close - self.entry_price) * self.position
            
            obs = np.array(
                [close, volume, rsi, float(self.position), pnl],
                dtype=np.float32
            )
            return obs
        else:
            return np.zeros(5, dtype=np.float32)
```

**Checklist:**
- [ ] File created: agent/rl/training_env.py
- [ ] Environment registered with gymnasium
- [ ] Reset/step methods tested with dummy data
- [ ] Risk gate callbacks integrated

**Estimate:** 2h

---

### T5.1.2 â€” Carregar Trade History

```python
# File: agent/rl/data_loader.py

import json
from pathlib import Path

def load_trade_history(filepath="data/trades_history.json"):
    """Carrega histÃ³rico de trades de Sprint 1 para ambiente."""
    with open(filepath, 'r') as f:
        trades = json.load(f)
    
    # Validar: cada trade tem entry, exit, qty
    for trade in trades:
        assert 'entry' in trade and 'exit' in trade and 'qty' in trade
    
    return trades

def convert_trades_to_ohlcv(trades):
    """Converte trades para sequÃªncia OHLCV para o ambiente."""
    # TODO: Flatten trade history â†’ OHLCV timeline
    ohlcv_list = []
    for trade in trades:
        # Simular: candle com close=entry, next candle close=exit
        ohlcv_list.append({
            'open': trade['entry'],
            'high': max(trade['entry'], trade['exit']),
            'low': min(trade['entry'], trade['exit']),
            'close': trade['exit'],
            'volume': trade.get('qty', 1) * 10,  # placeholder volume
        })
    return ohlcv_list
```

**Checklist:**
- [ ] loads trades_history.json
- [ ] Validates schema (entry, exit, qty)
- [ ] Converts to OHLCV for environment

**Estimate:** 1h

---

## ğŸ”„ Phase 2: PPO Training Loop (23 FEV 06:00-22:00 UTC)

**Owner:** The Brain (#3)  
**Total Time:** 16h wall-time (8h of 96h)

### T5.2.1 â€” Initialize PPO Agent

```python
# File: agent/rl/ppo_trainer.py

from stable_baselines3 import PPO
from agent.rl.training_env import CryptoTradingEnv

def create_ppo_agent(env, learning_rate=1e-4):
    """Cria agent PPO com params otimizados para trading."""
    
    policy_kwargs = dict(
        net_arch=[256, 256],  # 2 hidden layers, 256 neurons each
        activation_fn=torch.nn.ReLU,
    )
    
    ppo_model = PPO(
        "MlpPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=256,  # batch size
        batch_size=64,
        n_epochs=4,
        gamma=0.99,  # discount factor
        gae_lambda=0.95,
        policy_kwargs=policy_kwargs,
        verbose=1,
        tensorboard_log="logs/ppo_tensorboard/"
    )
    
    return ppo_model
```

**Checklist:**
- [ ] PPO model initialized
- [ ] Network architecture validated (256x256)
- [ ] Learning rate set to 1e-4
- [ ] TensorBoard logs configured

**Estimate:** 1h

---

### T5.2.2 â€” Training Loop with Daily Gates

```python
# File: agent/rl/training_loop.py

import json
from datetime import datetime
from stable_baselines3 import PPO
from backtest.metrics import MetricsCalculator

def train_with_daily_gates(
    env,
    ppo_model,
    total_timesteps=960000,  # 96h Ã· 360s/step â‰ˆ 960k steps
    checkpoint_interval=120000,  # save every 12h
    target_sharpe=1.0,
    early_stop_sharpe=1.0,
):
    """
    Treina PPO com gates diÃ¡rios de convergÃªncia.
    
    Daily Gates:
    - Day 1 (23 FEV): Sharpe â‰¥ 0.40 (ramp up)
    - Day 2 (24 FEV): Sharpe â‰¥ 0.70 (converging)
    - Day 3 (25 FEV): Sharpe â‰¥ 1.0 (target) or early stop
    """
    
    checkpoint_num = 0
    start_time = datetime.now()
    
    for timestep in range(0, total_timesteps, checkpoint_interval):
        # Train para prÃ³ximo checkpoint
        ppo_model.learn(
            total_timesteps=checkpoint_interval,
            log_interval=100,
        )
        
        checkpoint_num += 1
        
        # Save checkpoint
        checkpoint_path = f"models/ppo_checkpoint_{checkpoint_num}.pkl"
        ppo_model.save(checkpoint_path)
        
        # Daily gate validation
        metrics = evaluate_policy_sharpe(ppo_model, env)
        sharpe = metrics['sharpe_ratio']
        
        elapsed_hours = (datetime.now() - start_time).total_seconds() / 3600
        
        print(f"\n[Day {elapsed_hours/24:.1f}] Checkpoint {checkpoint_num}")
        print(f"  Sharpe Ratio: {sharpe:.3f}")
        print(f"  Win Rate: {metrics['win_rate']:.1%}")
        print(f"  Max DD: {metrics['max_drawdown']:.1%}")
        
        # EARLY STOP if Sharpe â‰¥ 1.0
        if sharpe >= early_stop_sharpe:
            print(f"\nâœ… EARLY STOP â€” Sharpe â‰¥ {early_stop_sharpe} reached!")
            ppo_model.save(f"models/ppo_v0_final.pkl")
            return ppo_model
        
        # Gate check (3 daily gates)
        day_num = int(elapsed_hours / 24) + 1
        if day_num == 1 and sharpe < 0.40:
            print("âš ï¸ Day 1 Gate FAIL â€” Sharpe < 0.40. Continuing...")
        elif day_num == 2 and sharpe < 0.70:
            print("âš ï¸ Day 2 Gate FAIL â€” Sharpe < 0.70. Continuing...")
        elif day_num == 3 and sharpe < 1.0:
            print("âš ï¸ Day 3 Gate FAIL â€” Sharpe < 1.0. Deadline approaching.")
    
    # Final save
    ppo_model.save("models/ppo_v0_final.pkl")
    return ppo_model

def evaluate_policy_sharpe(ppo_model, env, n_episodes=10):
    """Avalia policy em episÃ³dios e calcula Sharpe."""
    all_returns = []
    
    for _ in range(n_episodes):
        obs, _ = env.reset()
        episode_trades = []
        terminated = False
        
        while not terminated:
            action, _ = ppo_model.predict(obs, deterministic=True)
            obs, reward, terminated, _, _ = env.step(action)
            episode_trades.append(reward)
        
        all_returns.append(sum(episode_trades))
    
    # Calcular Sharpe dos returns
    returns_array = np.array(all_returns)
    sharpe = np.mean(returns_array) / (np.std(returns_array) + 1e-8)
    
    return {
        'sharpe_ratio': sharpe,
        'win_rate': sum(1 for r in all_returns if r > 0) / len(all_returns),
        'max_drawdown': 0.05,  # placeholder
    }
```

**Checklist:**
- [ ] Training loop implemented
- [ ] Daily gate validation (3 gates)
- [ ] Early stop @ Sharpe â‰¥ 1.0
- [ ] Checkpoints saved every 12h
- [ ] TensorBoard accessible at logs/

**Estimate:** 3h

---

## ğŸ¯ Phase 3: Final Validation (25 FEV 08:00-10:00 UTC)

**Owner:** Audit (#8)  
**Last 2 hours before deadline

### T5.3.1 â€” Backtest Validation

```python
# File: agent/rl/final_validation.py

from backtest.metrics import MetricsCalculator

def validate_trained_policy(policy_path, trade_data):
    """
    ValidaÃ§Ã£o final: roda policy treinada no backtest engine
    e verifica se todos mÃ©trica gates passam.
    """
    
    ppo_model = PPO.load(policy_path)
    env = CryptoTradingEnv(trade_data)
    
    obs, _ = env.reset()
    trades = []
    
    for _ in range(len(trade_data)):
        action, _ = ppo_model.predict(obs, deterministic=True)
        obs, reward, terminated, _, _ = env.step(action)
        
        if terminated:
            break
    
    # Calcular mÃ©tricas FINAIS
    calc = MetricsCalculator(
        env.trades_history,
        initial_capital=10000
    )
    
    metrics = {
        'sharpe': calc.calculate_sharpe_ratio(),
        'max_dd': calc.calculate_max_drawdown(),
        'win_rate': calc.calculate_win_rate(),
        'profit_factor': calc.calculate_profit_factor(),
        'consecutive_losses': calc.calculate_consecutive_losses(),
    }
    
    is_valid = calc.validate_against_thresholds(metrics)
    
    print("\n" + "="*50)
    print("TASK-005 FINAL VALIDATION")
    print("="*50)
    print(f"Sharpe Ratio: {metrics['sharpe']:.2f} (gate: â‰¥0.80)")
    print(f"Max Drawdown: {metrics['max_dd']:.1%} (gate: â‰¤12%)")
    print(f"Win Rate: {metrics['win_rate']:.1%} (gate: â‰¥45%)")
    print(f"Profit Factor: {metrics['profit_factor']:.2f} (gate: â‰¥1.5)")
    print(f"Max Losses: {metrics['consecutive_losses']} (gate: â‰¤5)")
    print("="*50)
    print(f"Result: {'âœ… PASS' if is_valid else 'âŒ FAIL'}")
    print("="*50)
    
    return is_valid, metrics
```

**Checklist:**
- [ ] Backtest validation script tested
- [ ] All 5 metrics gates verified
- [ ] TensorBoard logs reviewed
- [ ] Model saved to models/ppo_v0_final.pkl

**Estimate:** 1h

---

## ğŸ“Š Timeline (Wall-clock)

```
23 FEV 00:00 UTC â€” TASK-005 KICKOFF
â”œâ”€ 00:00-06:00 (6h)  â€” Phase 1 (Env setup)
â”œâ”€ 06:00-22:00 (16h) â€” Phase 2 (96h training Ã· 6 ~ 16h actual compute)
â””â”€ 22:00-24:00 (2h)  â€” Integration check

24 FEV 00:00-10:00 (10h) â€” Continued training

25 FEV 08:00-10:00 (2h) â€” Phase 3 (Final validation)

â° 25 FEV 10:00 UTC â€” DEADLINE (HARD)
â””â”€ Models saved + Metrics validated âœ…
```

---

## ğŸ Deliverables

| Item | Path | Owner | Status |
|------|------|-------|--------|
| Custom Env | agent/rl/training_env.py | Blueprint #7 | ğŸ“‹ |
| Data Loader | agent/rl/data_loader.py | Blueprint #7 | ğŸ“‹ |
| PPO Trainer | agent/rl/ppo_trainer.py | The Brain #3 | ğŸ“‹ |
| Daily Gates | agent/rl/training_loop.py | The Brain #3 | ğŸ“‹ |
| Final Model | models/ppo_v0_final.pkl | The Brain #3 | ğŸ“‹ |
| Validation | agent/rl/final_validation.py | Audit #8 | ğŸ“‹ |
| TensorBoard Logs | logs/ppo_tensorboard/ | Blueprint #7 | ğŸ“‹ |

---

## ğŸš€ Dependencies & Blockers

**Unblocked by:**
- âœ… Gate 3 (backtest/metrics) COMPLETE
- âœ… Sprint 1 trade history available
- âœ… stable-baselines3, gymnasium installed

**Blocks:**
- ğŸ”´ TASK-006 (Deployment) â€” waits for TASK-005 sign-off

---

## ğŸ“ Success Criteria

**All 6 must pass:**
1. âœ… Sharpe Ratio â‰¥ 0.80 (gate) / â‰¥ 1.20 (target)
2. âœ… Max Drawdown â‰¤ 12% (gate) / â‰¤ 10% (target)
3. âœ… Win Rate â‰¥ 45% (gate) / â‰¥ 55% (target)
4. âœ… Profit Factor â‰¥ 1.5 (gate) / â‰¥ 2.0 (target)
5. âœ… Consecutive Losses â‰¤ 5 (gate) / â‰¤ 3 (target)
6. âœ… Model saved: models/ppo_v0_final.pkl

**If all pass:** ğŸŸ¢ **GO-LIVE APPROVED** â†’ Production deployment ready

---

**Created:** 23 FEV 01:30 UTC  
**Owner:** The Brain (#3) + The Blueprint (#7)  
**Status:** ğŸš€ READY TO KICKOFF  
**Deadline:** â° 25 FEV 10:00 UTC (HARD)

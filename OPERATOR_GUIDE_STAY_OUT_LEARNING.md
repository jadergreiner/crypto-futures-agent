# [OPERADOR] Guia RÃ¡pido: Aprendizado "Ficar Fora do Mercado"

**Data**: 21 de fevereiro de 2026  
**VersÃ£o**: Round 5  
**Status**: âœ… Pronto para usar

---

## O Que Foi Feito?

O agente RL agora **aprende que ficar FORA do mercado Ã© tÃ£o importante quanto operar**.

Antes: O agente era incentivado a sempre estar operando ("algo Ã© melhor que nada")  
Depois: O agente aprende a escolher quando **NÃƒO** operar Ã© melhor

---

## Como Funciona?

### TrÃªs SituaÃ§Ãµes Onde Ficar Fora Gera Aprendizado

#### 1ï¸âƒ£ **Drawdown (Mercado em Queda)**
- **Quando**: Portfolio em drawdown â‰¥ 2%
- **AÃ§Ã£o**: Agente fica sem posiÃ§Ã£o aberta
- **Aprendizado**: +0.15 reward (proteÃ§Ã£o reconhecida)
- **Resultado**: Capital preservado durante crises

#### 2ï¸âƒ£ **MÃºltiplos Trades Recentes**
- **Quando**: 3+ operaÃ§Ãµes nas Ãºltimas 24h
- **AÃ§Ã£o**: Agente para e descansa
- **Aprendizado**: +0.10 reward (sabedoria reconhecida)
- **Resultado**: Evita "enfiar dinheiro perdido" (revenge trading)

#### 3ï¸âƒ£ **Inatividade Excessiva**
- **Quando**: Mais de 16 dias sem posiÃ§Ã£o
- **AÃ§Ã£o**: Agente sofre penalidade leve
- **Aprendizado**: -0.03 reward (precisa procurar oportunidades)
- **Resultado**: Evita totalmente adormecer

---

## Impacto PrÃ¡tico

Depois desse aprendizado, o agente deve:

âœ… **Menos operaÃ§Ãµes** (6-8 â†’ 3-4 por episÃ³dio)  
âœ… **Mais ganhos por trade** (1.2x â†’ 1.8x R-multiple)  
âœ… **Maior taxa de acertos** (45% â†’ 60%+)  
âœ… **Capital melhor protegido** (70% â†’ 85%+)  

**Analogia**: O agente aprende a ser um investidor paciente, nÃ£o um trader compulsivo.

---

## Como Usar

### Durante Training

O training continua normalmente:

```bash
python main.py --mode paper --train --train-epochs 100
```

O novo componente `r_out_of_market` estÃ¡ integrado automaticamente.

### Monitorar Aprendizado

Procure nos logs por:

```
Out-of-market bonus (drawdown protection): DD=2.50% > 2.0%
Out-of-market bonus (rest after losses): 4 trades recentes
Excess inactivity penalty: 150 candles sem posiÃ§Ã£o
```

Isso significa que o aprendizado estÃ¡ acontecendo âœ…

### Validar Funcionamento

Teste a implementaÃ§Ã£o:

```bash
python test_stay_out_of_market.py
```

Resultado esperado: **5/5 testes passando** âœ…

---

## Ajustar Comportamento

Se necessÃ¡rio, ajuste as constantes em `agent/reward.py`:

### Deixar Agente Mais Seletivo (Repousa Mais)

```python
OUT_OF_MARKET_LOSS_AVOIDANCE = 0.25      # De 0.15 (maior bonus)
OUT_OF_MARKET_THRESHOLD_DD = 1.5         # De 2.0 (mais sensÃ­vel)
```

â†’ Agente fica fora com mais frequÃªncia, menos operaÃ§Ãµes, wins maiores

### Deixar Agente Mais Agressivo (Opera Mais)

```python
EXCESS_INACTIVITY_PENALTY = 0.10         # De 0.03 (penalidade maior)
```

â†’ Agente busca mais oportunidades, mais operaÃ§Ãµes, risco maior

---

## DocumentaÃ§Ã£o TÃ©cnica

Para operadores tÃ©cnicos que queiram entender em detalhes:

- **`docs/LEARNING_STAY_OUT_OF_MARKET.md`** â€” 200+ linhas, explicaÃ§Ã£o completa
- **`IMPLEMENTATION_SUMMARY_STAY_OUT.md`** â€” SumÃ¡rio de implementaÃ§Ã£o
- **`test_stay_out_of_market.py`** â€” Testes automatizados (5 cenÃ¡rios)

---

## Perguntas Frequentes

### P: Isso vai tornar o agente mais lento?
**R**: NÃ£o. Menos operaÃ§Ãµes = mais ganhos. O agente fica mais eficiente.

### P: E se o mercado for bom, mas agente ficar fora?
**R**: Normal durante training. Depois de aprender bem, o agente sÃ³ fica fora quando realmente deve.

### P: Posso combinar isso com outro training?
**R**: Sim. O componente Ã© aditivo, compatÃ­vel com qualquer training anterior.

### P: Como medir se tÃ¡ funcionando?
**R**: Monitore:
- Logs com "Out-of-market bonus" â†’ âœ… Funciona
- Win rate aumentando â†’ âœ… Funciona
- R-multiple mÃ©dio crescendo â†’ âœ… Funciona

---

## PrÃ³ximos Passos

1. âœ… **Validar testes**: `python test_stay_out_of_market.py`
2. â³ **Treinar novo modelo**: `python main.py --train ...`
3. â³ **Monitorar aprendizado**: Observe os logs
4. â³ **Comparar mÃ©tricas**: Win rate, R-multiple, capital preservation

---

## SumÃ¡rio

O agente agora aprende a **inovaÃ§Ã£o mais importante do RL**: 

> **NÃ£o fazer nada no tempo certo Ã© melhor que fazer algo no tempo errado.**

Isso resultado em:
- Menos operaÃ§Ãµes
- Mais ganhos
- Capital melhor protegido
- Investidor mais inteligente

**Sucesso! ðŸš€**

